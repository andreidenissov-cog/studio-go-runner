// Copyright 2018-2021 (c) Cognizant Digital Business, Evolutionary AI. All rights reserved. Issued under the Apache 2.0 License.

package cuda

// This file contains the data structures used by the CUDA package that are used
// for when the platform is and is not supported

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"sort"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/dustin/go-humanize"
	"github.com/go-stack/stack"
	"github.com/jjeffery/kv" // MIT License
	"github.com/rs/xid"

	"github.com/lthibault/jitterbug"

	"github.com/mitchellh/copystructure"
)

type device struct {
	UUID       string    `json:"uuid"`
	Name       string    `json:"name"`
	Temp       uint      `json:"temp"`
	Powr       uint      `json:"powr"`
	Mem        uint64    `json:"mem"`
	EccFailure *kv.Error `json:"eccfailure"`
}

type cudaDevices struct {
	Devices []device `json:"devices"`
}

// GPUTrack is used to track usage of GPU cards and any kv.generated by the cards
// at the hardware level
//
type GPUTrack struct {
	UUID       string              // The UUID designation for the GPU being managed
	Slots      uint                // The number of logical slots the GPU based on its throughput/size has
	Mem        uint64              // The amount of memory the GPU has
	Allocated  bool                // Indicates is the card is allocated currently
	EccFailure *kv.Error           // Any Ecc failure related error messages, nil if no kv.encountered
	Tracking   map[string]struct{} // Used to validate allocations as they are released
}

type gpuTracker struct {
	Allocs map[string]*GPUTrack
	sync.Mutex
}

func (tracker *gpuTracker) Debug() (output string) {
	if tracker == nil {
		return output
	}
	byts, errGo := json.Marshal(tracker.Allocs)
	if errGo != nil {
		return kv.Wrap(errGo).With("stack", stack.Trace().TrimRuntime()).Error()
	}
	return string(byts) + " stack " + stack.Trace().TrimRuntime().String()
}

var (
	// A map keyed on the nvidia device UUID containing information about cards and
	// their occupancy by the go runner.
	//
	gpuAllocs gpuTracker

	// UseGPU is used for specific types of testing to disable GPU tests when there
	// are GPU cards potentially present but they need to be disabled, this flag
	// is not used during production to change behavior in any way
	UseGPU *bool

	// CudaInitErr records the result of the CUDA library initialization that would
	// impact ongoing operation
	CudaInitErr *kv.Error

	// CudaInitWarnings records warnings and kv.that are deemed not be be fatal
	// to the ongoing CUDA library usage but are of importance
	CudaInitWarnings = []kv.Error{}

	// CudaInTest is used to check if the running process is a go test process, if so then
	// this will disable certain types of checking when using very limited GPU
	// Hardware
	CudaInTest = false
)

func init() {
	temp := true
	UseGPU = &temp

	gpuDevices, err := getCUDAInfo()
	if err != nil {
		CudaInitErr = &err
		CudaInitWarnings = append(CudaInitWarnings, err)
		return
	}

	devs := os.Getenv("CUDA_VISIBLE_DEVICES")
	if len(devs) == 0 {
		devs = os.Getenv("NVIDIA_VISIBLE_DEVICES")
	}

	visDevices := strings.Split(devs, ",")

	if devs == "all" {
		visDevices = make([]string, 0, len(gpuDevices.Devices))
		for _, device := range gpuDevices.Devices {
			visDevices = append(visDevices, device.UUID)
		}
	}

	gpuAllocs.Lock()
	defer gpuAllocs.Unlock()
	gpuAllocs.Allocs = make(map[string]*GPUTrack, len(visDevices))

	// If the visDevices were specified use then to generate existing entries inside the device map.
	// These entries will then get filled in later.
	//
	// Look to see if we have any index values in here, it really should be all UUID strings.
	// Warn if we find some, but still continue.
	warned := false
	for _, id := range visDevices {
		if len(id) == 0 {
			continue
		}
		if i, err := strconv.Atoi(id); err == nil {
			if !warned {
				warned = true
				CudaInitWarnings = append(CudaInitWarnings, kv.NewError("CUDA_VISIBLE_DEVICES should be using UUIDs not indexes").With("stack", stack.Trace().TrimRuntime()))
			}
			if i > len(gpuDevices.Devices) {
				CudaInitWarnings = append(CudaInitWarnings, kv.NewError("CUDA_VISIBLE_DEVICES contained an index past the known population of GPU cards").With("stack", stack.Trace().TrimRuntime()))
			}
			gpuAllocs.Allocs[gpuDevices.Devices[i].UUID] = &GPUTrack{Tracking: map[string]struct{}{}}
		} else {
			gpuAllocs.Allocs[id] = &GPUTrack{Tracking: map[string]struct{}{}}
		}
	}

	if len(gpuAllocs.Allocs) == 0 {
		for _, dev := range gpuDevices.Devices {
			gpuAllocs.Allocs[dev.UUID] = &GPUTrack{Tracking: map[string]struct{}{}}
		}
	}

	// Scan the inventory, checking matches if they were specified in the visibility env var and then fill
	// in real world data
	//
	for _, dev := range gpuDevices.Devices {
		// Dont include devices that were not specified by CUDA_VISIBLE_DEVICES
		if _, isPresent := gpuAllocs.Allocs[dev.UUID]; !isPresent {
			fmt.Println("GPU Skipped", dev.UUID)
			continue
		}

		slots, err := GetSlots(dev.Name)
		if err != nil {
			CudaInitWarnings = append(CudaInitWarnings, err.With("gpu_uuid", dev.UUID))
		}

		gpuAllocs.Allocs[dev.UUID] = &GPUTrack{
			UUID:       dev.UUID,
			Slots:      slots,
			Mem:        dev.Mem,
			Allocated:  false,
			EccFailure: dev.EccFailure,
			Tracking:   map[string]struct{}{},
		}
	}
}

func GetCUDAInfo() (outDevs cudaDevices, err kv.Error) {
	return getCUDAInfo()
}

// GPUInventory can be used to extract a copy of the current state of the GPU hardware seen within the
// runner
func GPUInventory() (gpus []GPUTrack, err kv.Error) {
	return gpuAllocs.GPUInventory()
}

// GPUInventory can be used to extract a copy of the current state of the GPU hardware seen within the
// runner
func (allocs *gpuTracker) GPUInventory() (gpus []GPUTrack, err kv.Error) {

	gpus = []GPUTrack{}

	allocs.Lock()
	defer allocs.Unlock()

	for _, alloc := range allocs.Allocs {
		cpy, errGo := copystructure.Copy(*alloc)
		if errGo != nil {
			return nil, kv.Wrap(errGo).With("stack", stack.Trace().TrimRuntime())
		}
		gpus = append(gpus, cpy.(GPUTrack))
	}
	return gpus, nil
}

// MonitorGPUs will having initialized all of the devices in the tracking map
// when started as a go function check the devices for ECC and other kv.marking
// failed GPUs
//
func MonitorGPUs(ctx context.Context, statusC chan<- []string, errC chan<- kv.Error) {
	// Take all of the warnings etc that were gathered during initialization and
	// get them back to the error handling listener
	for _, warn := range CudaInitWarnings {
		select {
		case errC <- warn:
		case <-time.After(time.Second):
			// last gasp attempt to output the error
			fmt.Println(warn)
		}
	}

	firstTime := true

	t := jitterbug.New(time.Second*30, &jitterbug.Norm{Stdev: time.Second * 3})
	defer t.Stop()

	for {
		select {
		case <-t.C:
			// If cuda support is compiled out dont bother continuing resources
			// that dont exist
			if !HasCUDA() {
				continue
			}
			gpuDevices, err := getCUDAInfo()
			if err != nil {
				select {
				case errC <- err:
				default:
					// last gasp attempt to output the error
					fmt.Println(err)
				}
			}
			// Look at allhe GPUs we have in our hardware config
			for _, dev := range gpuDevices.Devices {
				if firstTime {
					msg := []string{"gpu found", "name", dev.Name, "uuid", dev.UUID, "stack", stack.Trace().TrimRuntime().String()}
					select {
					case statusC <- msg:
					case <-time.After(time.Second):
						fmt.Println(msg)
					}
				} else {
					msg := gpuAllocs.DebugMsg()

					select {
					case statusC <- []string{msg}:
					case <-time.After(time.Second):
						fmt.Println(msg)
					}
				}
				if dev.EccFailure != nil {
					gpuAllocs.MarkFailure(dev.UUID, dev.EccFailure)
					select {
					case errC <- *dev.EccFailure:
					default:
						// last gasp attempt to output the error
						fmt.Println(dev.EccFailure)
					}
				}
			}
			firstTime = false
		case <-ctx.Done():
			return
		}
	}
}

func (allocs *gpuTracker) MarkFailure(uuid string, devErr *kv.Error) {
	allocs.Lock()
	// Check to see if the hardware GPU had a failure
	// and if it is in the tracking table and does
	// not yet have an error logged log the error
	// in the tracking table
	if gpu, isPresent := allocs.Allocs[uuid]; isPresent {
		if gpu.EccFailure == nil {
			gpu.EccFailure = devErr
			allocs.Allocs[gpu.UUID] = gpu
		}
	}
	allocs.Unlock()
}

func (allocs *gpuTracker) DebugMsg() (msg string) {
	allocs.Lock()
	defer allocs.Unlock()

	return allocs.Debug()
}

// GPUCount returns the number of allocatable GPU resources
func GPUCount() (cnt uint) {
	return gpuAllocs.GPUCount()
}

// GPUCount returns the number of allocatable GPU resources
func (allocs *gpuTracker) GPUCount() (cnt uint) {
	allocs.Lock()
	defer allocs.Unlock()

	return uint(len(allocs.Allocs))
}

// GPUSlots gets the free and total number of GPU capacity slots within
// the machine
//
func GPUSlots() (cnt uint, freeCnt uint) {
	return gpuAllocs.GPUSlots()
}

func (allocs *gpuTracker) GPUSlots() (cnt uint, freeCnt uint) {
	allocs.Lock()
	defer allocs.Unlock()

	for _, alloc := range allocs.Allocs {
		cnt += alloc.Slots
		freeCnt += alloc.Slots
	}
	return cnt, freeCnt
}

// LargestFreeGPUSlots gets the largest number of single device free GPU slots
//
func LargestFreeGPUSlots() (cnt uint) {
	return gpuAllocs.LargestFreeGPUSlots()
}

func (allocs *gpuTracker) LargestFreeGPUSlots() (cnt uint) {
	allocs.Lock()
	defer allocs.Unlock()

	for _, alloc := range allocs.Allocs {
		if alloc.Slots > cnt {
			if !alloc.Allocated {
				cnt = alloc.Slots
			}
		}
	}
	return cnt
}

// TotalFreeGPUSlots gets the largest number of single device free GPU slots
//
func TotalFreeGPUSlots() (cnt uint) {
	return gpuAllocs.TotalFreeGPUSlots()
}

func (allocs *gpuTracker) TotalFreeGPUSlots() (cnt uint) {
	allocs.Lock()
	defer allocs.Unlock()

	for _, alloc := range allocs.Allocs {
		if !alloc.Allocated {
			cnt += alloc.Slots
		}
	}
	return cnt
}

// LargestFreeGPUMem will obtain the largest number of available GPU slots
// on any of the individual cards accessible to the runner
func LargestFreeGPUMem() (freeMem uint64) {
	return gpuAllocs.LargestFreeGPUMem()
}

func (allocs *gpuTracker) LargestFreeGPUMem() (freeMem uint64) {
	allocs.Lock()
	defer allocs.Unlock()

	for _, alloc := range allocs.Allocs {
		if !alloc.Allocated {
			if alloc.Slots != 0 && alloc.Mem > freeMem {
				freeMem = alloc.Mem
			}
		}
	}
	return freeMem
}

// GPUAllocated is used to record the allocation/reservation of a GPU resource on behalf of a caller
//
type GPUAllocated struct {
	tracking string            // Allocation tracking ID
	uuid     string            // The device identifier this allocation was successful against
	Slots    uint              // The number of GPU slots given from the allocation
	Mem      uint64            // The amount of memory given to the allocation
	Env      map[string]string // Any environment variables the device allocator wants the runner to use
}

// GPUAllocations records the allocations that together are present to a caller.
//
type GPUAllocations []*GPUAllocated

// AllocGPU will select the default allocation pool for GPUs and call the allocation for it.
//
func AllocGPU(maxGPU uint, maxGPUMem uint64, unitsOfAllocation []int, cardCount int, live bool) (alloc GPUAllocations, err kv.Error) {
	return gpuAllocs.AllocGPU(maxGPU, maxGPUMem, unitsOfAllocation, cardCount, live)
}

// AllocGPU will attempt to find a free CUDA capable GPU from a supplied allocator pool
// and assign it to the client.  It will on finding a device set the appropriate values
// in the allocated return structure that the client can use to manage their resource
// consumption to match the permitted limits.
//
// Any allocations will take an entire card, we do not break cards across experiments.
//
// This receiver uses a user supplied pool which allows for unit tests to be written that use a
// custom pool
//
// The live parameter if false can be used to test if the allocation would be successful
// without performing it.  If live false is used no allocation will be returned and err will be nil
// if the allocation have been successful.
//
func (allocator *gpuTracker) AllocGPU(maxGPU uint, maxGPUMem uint64, units []int, cardCount int, live bool) (alloc GPUAllocations, err kv.Error) {

	alloc = GPUAllocations{}

	if maxGPU == 0 && maxGPUMem == 0 {
		return alloc, nil
	}

	// Sort the user supplied slots so that we can select from the range of slots
	// on the card so that we dont take cards that are too excessive for smaller jobs
	// etc
	sort.Slice(units, func(i, j int) bool { return units[i] < units[j] })

	// Start building logging style information to be used in the
	// event of a real error
	kvDetails := []interface{}{"maxGPU", maxGPU, "", humanize.Bytes(maxGPUMem)}
	kvDetails = append(kvDetails, []interface{}{"allocs", allocator.Debug()}...)

	// Now we lock after doing initialization of the functions own variables
	allocator.Lock()
	defer allocator.Unlock()

	// Add a structure that will be used later to order our UUIDs
	// by the number of slots they have
	type SlotsByUUID struct {
		uuid  string
		slots uint
		mem   uint64
	}
	slotsByUUID := make([]SlotsByUUID, 0, len(allocator.Allocs))

	// Take any unallocated cards that have the exact number of slots that we have
	// in our permitted units and use those, but exclude cards with
	// ECC errors
	usableAllocs := make(map[string]*GPUTrack, len(allocator.Allocs))
	for k, v := range allocator.Allocs {

		// Card is busy and in use
		if v.Allocated {
			continue
		}

		// Cannot use this cards it is broken
		if v.EccFailure != nil {
			continue
		}
		// Check memory
		if v.Mem < maxGPUMem && maxGPUMem != 0 {
			fmt.Println("NOT ENOUGH MEM:", v.Mem, "request: ", maxGPUMem)
			continue
		}

		// Make sure the units contains the value of the valid range of slots
		// acceptable to the caller
		pos := sort.SearchInts(units, int(v.Slots))
		if pos < len(units) && int(v.Slots) == units[pos] {
			usableAllocs[k] = v
			slotsByUUID = append(slotsByUUID, SlotsByUUID{uuid: v.UUID, slots: v.Slots, mem: v.Mem})
		} else {
			fmt.Println("NO SLOTS:", v.Slots, "units: ", units)
		}
	}

	if len(slotsByUUID) < cardCount {
		fmt.Println("NO GPUs:", len(slotsByUUID), "request: ", cardCount)
		return nil, kv.NewError("insufficient free GPUs").With(kvDetails...)
	}

	// Take the permitted cards and sort their UUIDs in order of the
	// smallest number of slots first
	sort.Slice(slotsByUUID, func(i, j int) bool {
		if slotsByUUID[i].slots > slotsByUUID[j].slots {
			return false
		}

		return slotsByUUID[i].uuid < slotsByUUID[j].uuid
	})

	kvDetails = append(kvDetails, []interface{}{"slots", slotsByUUID})

	slotsByUUID = slotsByUUID[:cardCount]

	// Got as far as knowing the allocation will work so check for the live flag
	if !live {
		return nil, nil
	}

	// Go through the chosen combination of cards and do the allocations
	//
	for _, detail := range slotsByUUID {
		allocator.Allocs[detail.uuid].Allocated = true

		tracking := xid.New().String()
		alloc = append(alloc, &GPUAllocated{
			tracking: tracking,
			uuid:     detail.uuid,
			Slots:    detail.slots,
			Mem:      detail.mem,
			Env: map[string]string{
				"NVIDIA_VISIBLE_DEVICES": detail.uuid,
				"CUDA_VISIBLE_DEVICES":   detail.uuid,
			},
		})
		allocator.Allocs[detail.uuid].Tracking[tracking] = struct{}{}
	}

	return alloc, nil
}

// ReturnGPU releases the GPU allocation passed in.  It will validate some of the allocation
// details but is an honors system.
//
func ReturnGPU(alloc *GPUAllocated) (err kv.Error) {
	return gpuAllocs.ReturnGPU(alloc)
}

func (allocator *gpuTracker) ReturnGPU(alloc *GPUAllocated) (err kv.Error) {

	if alloc.Slots == 0 {
		return nil
	}

	allocator.Lock()
	defer allocator.Unlock()

	// Make sure that the allocation is still valid
	if _, isPresent := allocator.Allocs[alloc.uuid]; !isPresent {
		return kv.NewError("cuda device no longer in service").With("device", alloc.uuid).With("stack", stack.Trace().TrimRuntime())
	}

	if _, isPresent := allocator.Allocs[alloc.uuid].Tracking[alloc.tracking]; !isPresent {
		return kv.NewError("invalid allocation").With("alloc_id", alloc.tracking).With("stack", stack.Trace().TrimRuntime())
	}

	delete(allocator.Allocs[alloc.uuid].Tracking, alloc.tracking)

	// Release the trackign structuture for others to use
	allocator.Allocs[alloc.uuid].Allocated = false

	return nil
}
